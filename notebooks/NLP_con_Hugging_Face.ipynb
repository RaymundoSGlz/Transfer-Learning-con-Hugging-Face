{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgUNO42S9x3i"
      },
      "source": [
        "# NLP con Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPUO9PylwUcr"
      },
      "source": [
        "## Procesando los datos para NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxAWeUN9zsaT"
      },
      "source": [
        "### Descargando el dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrDuinME8a61"
      },
      "source": [
        "Usaremos el dataset MRPC. Este es uno de los 10 datasets que componen el [benchmark (punto de referencia) GLUE](https://huggingface.co/datasets/glue). Se utiliza para medir el rendimiento de los modelos ML en 10 tareas de clasificación de texto diferentes.\n",
        "\n",
        "En otras palabras, seleccionamos el subset `mrpc` del dataset `glue`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sGMyV3iIdTUh"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"glue\", \"mrpc\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLRhoTyvBb_c"
      },
      "source": [
        "Así se ve un ejemplo. Notamos que `mrpc` está compuesto de dos oraciones y una etiqueta que indica si los dos enunciados son equivalentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "A7xTuFa4zegd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'sentence1': 'U.S. Agriculture Secretary Ann Veneman , who announced Tuesdays ban , also said Washington would send a technical team to Canada to help .',\n",
              " 'sentence2': \"U.S. Agriculture Secretary Ann Veneman , who announced yesterday 's ban , also said Washington would send a technical team to Canada to assist in the Canadian situation .\",\n",
              " 'label': 1,\n",
              " 'idx': 446}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ex = ds[\"train\"][400]\n",
        "ex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "t_QPFmoZBmdR"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ClassLabel(names=['not_equivalent', 'equivalent'], id=None)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels = ds[\"train\"].features[\"label\"]\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "B74qToumCm-k"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'equivalent'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels.int2str(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_At65Vv0Hmz"
      },
      "source": [
        "### Tokenizando\n",
        "\n",
        "¿Recuerdas que con visión descargamos el feature extractor directamente del repositorio del modelo pre-entrenado que vamos a usar como base?\n",
        "\n",
        "Podemos pensar en la función tokenizadora como el equivalente en el NLP.\n",
        "\n",
        "Descargamos el tokenizador directamente del repo del modelo que usaremos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "j02Vp1mo0hVq"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Modelo\n",
        "repo_id = \"bert-base-uncased\"\n",
        "\n",
        "# Tokenizador\n",
        "tokenizer = AutoTokenizer.from_pretrained(repo_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUqXIm4fDi3W"
      },
      "source": [
        "Para preprocesar el conjunto de datos necesitamos convertir el texto en números que el modelo pueda entender. Esto se hace con un tokenizador.\n",
        "\n",
        "Pasar de texto a números se conoce como codificación o encoding. El encoding se realiza en un proceso de dos pasos: la tokenización, seguida de la conversión a input ids. Por el momento nos basta saber que estamos traduciendo texto a números llamados como input ids. Estos estarán en el formato adecuado para alimentar nuestro modelo.\n",
        "\n",
        "Podemos alimentar al tokenizador con una oración o una lista de oraciones, por lo que podemos tokenizar directamente todas las primeras oraciones y todas las segundas oraciones de cada par de esta manera:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "huibMGKOD-Ut"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [101, 2027, 2018, 2405, 2019, 15147, 2006, 1996, 4274, 2006, 2238, 2184, 1010, 5378, 1996, 6636, 2005, 5096, 1010, 2002, 2794, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_sentence_1 = tokenizer(ds[\"train\"][\"sentence1\"][2])\n",
        "tokenized_sentence_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBrkzA08EslP"
      },
      "source": [
        "Necesitamos manejar los dos enunciados como un par y no separados. El tokenizador puede tomar un par de secuencias y prepararlas de la manera que espera nuestro modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3O7kBG_bE18h"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [101, 2023, 2003, 1037, 7099, 6251, 102, 2178, 7099, 6251, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# tokenizamos dos frases\n",
        "inputs = tokenizer(\"This is a sample sentence\", \"Another sample sentence\")\n",
        "inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOzgBI7nGC31"
      },
      "source": [
        "¿Qué significa cada uno de los valores que nos retorna el tokenizador?\n",
        "- `input_ids` es la traducción de palabras a números.\n",
        "- `attention_mask` es un tensor con la misma forma que `input_ids`, pero lleno de 0 y 1: los 1 indican que se debe atender a los tokens correspondientes y los 0 indican que no se deben atender. Es decir, deben ser ignorados por el modelo.\n",
        "- `token_type_ids` dice al modelo qué parte de la entrada es la primera oración y cuál es la segunda oración."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7ewj-TxGj4P"
      },
      "source": [
        "El modelo espera que las entradas sean de la forma [CLS] oración 1 [SEP] oración 2 [SEP] cuando hay dos oraciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "w6IL1vRNjehn"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'this',\n",
              " 'is',\n",
              " 'a',\n",
              " 'sample',\n",
              " 'sentence',\n",
              " '[SEP]',\n",
              " 'another',\n",
              " 'sample',\n",
              " 'sentence',\n",
              " '[SEP]']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOzRDDXMG6KS"
      },
      "source": [
        "Si seleccionamos otro modelo en el Hub no necesariamente tendremos `token_type_ids` en las entradas tokenizadas (por ejemplo, no se devuelven si usa un modelo `DistilBERT`). Solo se devuelven cuando el modelo sabrá qué hacer con ellas, porque los ha visto durante su preentrenamiento.\n",
        "\n",
        "En general, no necesitamos preocuparnos por si hay o no `token_type_ids` en nuestras entradas tokenizadas, siempre que usemos el tokenizador correspondiente al modelo, todo estará bien ya que el tokenizador sabe qué proporcionar al modelo.\n",
        "\n",
        "Por ejemplo, durante esta clase utilizaremos un modelo [`distilroberta-base`](https://huggingface.co/distilroberta-base) por su tamaño y efectividad. Pero no cuenta con `token_type_ids` y aún así nos regresa excelentes resultados.\n",
        "\n",
        "En la organización del Platzi en el Hub puedes encontrar un [modelo BERT](https://huggingface.co/platzi/platzi-distilroberta-base-mrpc-glue-omar-espejel) afinado siguiendo el mismo proceso que usamos en esta clase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kUn74FigzhHm"
      },
      "outputs": [],
      "source": [
        "# Modelo\n",
        "repo_id = \"distilroberta-base\"\n",
        "\n",
        "# Tokenizador\n",
        "tokenizer = AutoTokenizer.from_pretrained(repo_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlcHOXcQ2-kW"
      },
      "source": [
        "Creamos una función tokenizadora. Recibe un ejemplo y lo tokeniza."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1hkNcAjX0nSJ"
      },
      "outputs": [],
      "source": [
        "def tokenize_fn(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"sentence1\"], examples[\"sentence2\"], # frases\n",
        "        truncation=True # truncamos las frases al máximo permitido\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "49j2GVph0swg"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28bc73dedd14467c9281d9b350b337a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# tokenizamos el dataset\n",
        "prepared_ds = ds.map(tokenize_fn, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIsjhHyf3O_L"
      },
      "source": [
        "### Definiendo el data collator: Dynamic padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIH2CWjNyQ13"
      },
      "source": [
        "Necesitamos que nuestros tensores tengan una forma rectangular. Es decir que tengan el mismo tamaño cada uno de los ejemplos. Sin embargo, los textos no necesariamente tienen el mismo tamaño.\n",
        "\n",
        "Para ello usamos el relleno o padding. El padding se asegura de que todas nuestras oraciones tengan la misma longitud al agregar una palabra especial llamada padding token a las oraciones con menos valores. Por ejemplo, si tenemos 10 oraciones con 10 palabras y 1 oración con 20 palabras, el relleno garantizará que todas las oraciones tengan 20 palabras.\n",
        "\n",
        "Dejamos el argumento de `padding` del tokenizer vacío en nuestra función de tokenización por ahora. Esto se debe a que rellenar (hacer padding) todas las muestras hasta la longitud máxima del dataset no es eficiente, es mejor rellenar las muestras cuando estamos construyendo un batch, ya que entonces solo necesitamos rellenar hasta la longitud máxima en ese batch, y no la longitud máxima en todo el dataset. ¡Esto puede ahorrar mucho tiempo y potencia de procesamiento cuando las entradas tienen longitudes muy variables!\n",
        "\n",
        "Usaremos un DataCollator para esto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHKCmEdneI7X"
      },
      "source": [
        "Rellenemos (hagamos padding) todos los ejemplos con la longitud del elemento más largo del batch. A esta técnica se le conoce como relleno dinámico o dynamic padding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "JMjUzWDK3SzH"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "# creamos el collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpONY8Hl3Yfb"
      },
      "source": [
        "## Entrenamiento y evaluación\n",
        "\n",
        "Definamos el resto de los argumentos necesarios para `Trainer`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlfIjQnh3bhn"
      },
      "source": [
        "### Definiendo la métrica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Z_h5Pm9e3ZGi"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "# Metrica\n",
        "def compute_metrics(eval_pred):\n",
        "    metric = evaluate.load(\"glue\", \"mrpc\") # cargamos la metrica\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1) # obtenemos las predicciones\n",
        "    return metric.compute(predictions=predictions, references=labels) # calculamos la metrica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erd8wVZ13l-8"
      },
      "source": [
        "### Configurando `Trainer`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "dy9gQdTj3q1q"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# Etiquetas\n",
        "labels = ds[\"train\"].features[\"label\"].names\n",
        "\n",
        "# Cargamos el modelo\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    repo_id, # id del modelo\n",
        "    num_labels = len(labels), # numero de etiquetas\n",
        "    id2label = {str(i): c for i, c in enumerate(labels)}, # id a etiqueta\n",
        "    label2id = {c: str(i) for i, c in enumerate(labels)} # etiqueta a id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DW93t2gG4luP"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "# Argumentos de entrenamiento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = \"/RaymundoSGlz/distilroberta-base-mrpc-glue\", # ruta donde se guardaran los checkpoints\n",
        "    evaluation_strategy = \"steps\", # evaluamos por pasos\n",
        "    num_train_epochs = 3, # numero de epocas\n",
        "    push_to_hub = True, # subimos el modelo al hub\n",
        "    load_best_model_at_end = True # cargamos el mejor modelo al final\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTZJl_b_6AvW"
      },
      "source": [
        "### Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fyrp6fmGy3e0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/RaymundoSGlz/distilroberta-base-mrpc-glue is already a clone of https://huggingface.co/RaymundoSGlz/distilroberta-base-mrpc-glue. Make sure you pull the latest changes with `repo.git_pull()`.\n"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "# Creamos el trainer\n",
        "trainer = Trainer(\n",
        "    model = model, # modelo\n",
        "    args = training_args, # argumentos de entrenamiento\n",
        "    train_dataset = prepared_ds[\"train\"], # dataset de entrenamiento\n",
        "    eval_dataset = prepared_ds[\"validation\"], # dataset de validacion\n",
        "    data_collator = data_collator, # collator\n",
        "    tokenizer = tokenizer, # tokenizador\n",
        "    compute_metrics = compute_metrics # metrica\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "KX7GIg8b6Wsn"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/.local/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1377/1377 03:42, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.490900</td>\n",
              "      <td>0.544765</td>\n",
              "      <td>0.860294</td>\n",
              "      <td>0.899471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.314800</td>\n",
              "      <td>0.675343</td>\n",
              "      <td>0.843137</td>\n",
              "      <td>0.887324</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b42c485d47a433c81a0203363e05858",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/5.75k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Several commits (2) will be pushed upstream.\n",
            "The progress bars may be unreliable.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b18d1e51aea4a2a83d3141eecb87519",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload file pytorch_model.bin:   0%|          | 1.00/313M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "To https://huggingface.co/RaymundoSGlz/distilroberta-base-mrpc-glue\n",
            "   3f27199..44c713d  main -> main\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  total_flos               =   191920GF\n",
            "  train_loss               =     0.3433\n",
            "  train_runtime            = 0:03:44.27\n",
            "  train_samples_per_second =     49.065\n",
            "  train_steps_per_second   =       6.14\n"
          ]
        }
      ],
      "source": [
        "# Entrenamos\n",
        "train_results = trainer.train()\n",
        "# Guardamos el modelo\n",
        "trainer.save_model()\n",
        "# Mostramos las metricas\n",
        "trainer.log_metrics(\"train\", train_results.metrics)\n",
        "# Guardamos las metricas\n",
        "trainer.save_metrics(\"train\", train_results.metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZ-2BxDN6TjY"
      },
      "source": [
        "### Evaluación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [51/51 00:01]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.8603\n",
            "  eval_f1                 =     0.8995\n",
            "  eval_loss               =     0.5448\n",
            "  eval_runtime            = 0:00:02.99\n",
            "  eval_samples_per_second =    136.236\n",
            "  eval_steps_per_second   =     17.029\n"
          ]
        }
      ],
      "source": [
        "# Evaluamos\n",
        "metrics = trainer.evaluate(prepared_ds[\"validation\"])\n",
        "# Mostramos las metricas\n",
        "trainer.log_metrics(\"eval\", metrics)\n",
        "# Guardamos las metricas\n",
        "trainer.save_metrics(\"eval\", metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPxunFyM6fLy"
      },
      "source": [
        "### Compartimos en el Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fowyC7_56W0S"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "To https://huggingface.co/RaymundoSGlz/distilroberta-base-mrpc-glue\n",
            "   44c713d..00f3cf5  main -> main\n",
            "\n",
            "To https://huggingface.co/RaymundoSGlz/distilroberta-base-mrpc-glue\n",
            "   00f3cf5..d0fb970  main -> main\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'https://huggingface.co/RaymundoSGlz/distilroberta-base-mrpc-glue/commit/00f3cf529c92b269642dabba10e2b2250fb36b2f'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "kwargs = {\n",
        "    \"finetuned_from\": model.config.name_or_path, # modelo base\n",
        "    \"tasks\": \"text-classification\", # tareas\n",
        "    \"dataset\": [\"glue\", \"mrpc\"], # dataset\n",
        "    \"tags\": [\"text-classification\", \"glue\", \"mrpc\"] # tags\n",
        "}\n",
        "\n",
        "# Subimos el modelo al hub\n",
        "trainer.push_to_hub(\n",
        "    commit_message = \"Entrenamiento de MRPC con DistilRoberta\", # mensaje del commit\n",
        "    **kwargs # argumentos\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
